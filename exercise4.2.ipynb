{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUfAl8Td3OdzFJpzM9nU2m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Day 4 Exercise: Data Warehousing, Data Modeling for Fraud Analytics & DevOps/CI-CD (Practical)\n","\n","## Scenario\n","Building on the fraud detection rules implemented in PySpark from Day 3, you now need to operationalize the data pipeline by automating its deployment. The enriched transaction data and fraud flags (`sdf_final_transactions`) are stored in a data lake in CSV format. Your task is to create a CI/CD pipeline using GitHub Actions to deliver your PySpark code to a DEV environment and an Ansible playbook to deploy a basic infrastructure to support your pipeline. The focus is on practical implementation while ensuring secrets are protected and adhering to best practices for a robust deployment process.\n","\n","## Data to Use ðŸ“Š\n","You will use the conceptual output from the Day 3 exercise: a PySpark DataFrame (`sdf_final_transactions`) containing enriched transaction data with fraud flags (`is_fraudulent_rule1`, `is_fraudulent_rule2`). The data is stored in a data lake in CSV format (e.g., `dbfs:/mnt/datalake/fraud_detection/sdf_final_transactions/`).\n","\n","## Tasks\n","\n","### Part 1: CI/CD Pipeline with GitHub Actions (Practical)\n","**Objective**: Create a GitHub Actions workflow to automate the build, test, and deployment of your PySpark fraud detection script to an Azure Databricks DEV environment. Ensure secrets (e.g., Databricks token, storage credentials) are securely handled.\n","\n","**Tasks**:\n","1. **GitHub Actions Workflow**:\n","   - Create a `.yml` file for a GitHub Actions workflow that triggers on push to a feature branch or pull request to `main`.\n","   - Include the following stages:\n","     - **Linting**: Check Python code for style and errors using `flake8`.\n","     - **Unit Testing**: Run unit tests for your PySpark script using `pytest`.\n","     - **Build**: Package the Python script and dependencies (e.g., create a `requirements.txt`).\n","     - **Deploy to DEV**: Deploy the script to an Azure Databricks workspace in the DEV environment using the Databricks CLI or API.\n","   - Use GitHub Secrets to securely store sensitive information like Databricks tokens and Azure Data Lake Storage credentials.\n","   - Ensure the workflow outputs logs for debugging but does not expose secrets.\n","\n","2. **Project Structure**:\n","   - Structure your Git repository with the following:\n","     - `/src`: Contains the main PySpark script (`fraud_detection.py`).\n","     - `/tests`: Contains unit tests (`test_fraud_detection.py`).\n","     - `/requirements.txt`: Lists dependencies (e.g., `pyspark`, `pytest`, `flake8`).\n","     - `.gitignore`: Excludes sensitive files (e.g., `.env`, Databricks token files, temporary files).\n","   - Provide the content of `.gitignore` and `requirements.txt`.\n","\n","3. **Unit Testing**:\n","   - Write a sample unit test in `test_fraud_detection.py` to verify one fraud rule (e.g., `is_fraudulent_rule1` for high transaction value for new customers).\n","   - Use a small, mocked dataset to simulate `sdf_final_transactions`.\n","\n","### Part 2: Ansible Playbook for Infrastructure Deployment (Practical)\n","**Objective**: Create an Ansible playbook to deploy a basic infrastructure to support your PySpark fraud detection pipeline in the DEV environment. The infrastructure includes an Azure Databricks workspace and connectivity to an Azure Data Lake Storage account.\n","\n","**Tasks**:\n","1. **Ansible Playbook**:\n","   - Create an Ansible playbook (`deploy_databricks_infra.yml`) to:\n","     - Provision an Azure Databricks workspace in the DEV environment (or configure an existing one).\n","     - Configure a mount point to the Azure Data Lake Storage account where the CSV data resides.\n","     - Set up a Databricks cluster with basic configurations (e.g., single node, Spark version compatible with your PySpark script).\n","   - Use Ansible vault to encrypt sensitive variables (e.g., Azure service principal credentials, Databricks workspace tokens).\n","   - Ensure the playbook is idempotent (can be run multiple times without errors).\n","\n","2. **Inventory and Variables**:\n","   - Provide an Ansible inventory file (`inventory.yml`) defining the target environment (e.g., localhost for Azure CLI execution).\n","   - Provide a variable file (`vars.yml`) for non-sensitive configurations (e.g., Databricks workspace name, cluster settings).\n","   - Describe how to use Ansible vault for sensitive variables (e.g., `ansible-vault encrypt_string` for Azure credentials).\n","\n","3. **Assumptions**:\n","   - Assume the Azure CLI is installed on the machine running Ansible.\n","   - Assume the Azure Data Lake Storage account already exists, and you are mounting it to Databricks.\n","   - Assume the CSV data is accessible at `dbfs:/mnt/datalake/fraud_detection/`.\n","\n","### Deliverables\n","1. **GitHub Actions Workflow**:\n","   - A `.yml` file for the CI/CD pipeline.\n","   - A `.gitignore` file.\n","   - A `requirements.txt` file.\n","   - A sample unit test file (`test_fraud_detection.py`).\n","2. **Ansible Playbook**:\n","   - An Ansible playbook (`deploy_databricks_infra.yml`).\n","   - An inventory file (`inventory.yml`).\n","   - A variable file (`vars.yml`).\n","   - Instructions for encrypting sensitive variables using Ansible vault.\n","3. **Markdown Explanation**:\n","   - Explain your CI/CD pipeline design, including how secrets are protected.\n","   - Describe the Ansible playbook structure and how it ensures idempotency.\n","   - Discuss31. **Code Quality**: Readability, modularity, proper error handling, and adherence to PEP 8 standards.\n","32. **Robustness**: The pipeline and playbook handle edge cases like missing dependencies, failed API calls, or existing resources.\n","33. **Efficiency**: The pipeline minimizes runtime, and the playbook optimizes resource provisioning.\n","34. **Reasoning**: Clear and logical explanations in the Markdown file, covering all tasks and edge cases."],"metadata":{"id":"rYmDBX345WNW"}}]}