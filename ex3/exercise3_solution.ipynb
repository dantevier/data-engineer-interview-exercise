{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1OA9i3BPv8eELTFOps8db2m025yohYcq5","authorship_tag":"ABX9TyMi2o0ZkNYK8NkOMps+HAhr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import lag, count, col, when, lit, unix_timestamp, to_timestamp"],"metadata":{"id":"L7wNv5JH8oF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the data from the CSV file into a pandas DataFrame\n","try:\n","    df_enriched_transactions = pd.read_csv('/data/ex2-df_enriched_transaction.csv')\n","    print(\"Pandas DataFrame loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: The file 'ex2-df_enriched_transaction.csv' was not found.\")\n","    df_enriched_transactions = None # Set to None if file not found to prevent further errors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxsvM26171vI","executionInfo":{"status":"ok","timestamp":1752935144720,"user_tz":-120,"elapsed":44,"user":{"displayName":"Roberto Cerrone","userId":"14489044185171455121"}},"outputId":"be2bbfc1-e461-4776-90fe-a18adff0058b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pandas DataFrame loaded successfully.\n"]}]},{"cell_type":"code","source":["if df_enriched_transactions is not None:\n","    # Initialize SparkSession\n","    spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n","    print(\"SparkSession initialized successfully.\")\n","\n","    # Create PySpark DataFrame from Pandas DataFrame\n","    sdf_enriched_transactions = spark.createDataFrame(df_enriched_transactions)\n","    print(\"PySpark DataFrame created successfully.\")\n","\n","    # Show Schema and Data\n","    print(\"PySpark DataFrame Schema:\")\n","    sdf_enriched_transactions.printSchema()\n","\n","    print(\"\\nFirst 5 rows of the PySpark DataFrame:\")\n","    sdf_enriched_transactions.show(5)\n","else:\n","    print(\"Could not proceed with Spark operations due to file loading error.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"xhbViRco8XS4","executionInfo":{"status":"ok","timestamp":1752935145039,"user_tz":-120,"elapsed":319,"user":{"displayName":"Roberto Cerrone","userId":"14489044185171455121"}},"outputId":"4382d1be-74da-4054-cba3-375159123513"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SparkSession initialized successfully.\n","PySpark DataFrame created successfully.\n","PySpark DataFrame Schema:\n","root\n"," |-- transaction_id: string (nullable = true)\n"," |-- customer_id: string (nullable = true)\n"," |-- amount: double (nullable = true)\n"," |-- timestamp: string (nullable = true)\n"," |-- currency: string (nullable = true)\n"," |-- ip_address: string (nullable = true)\n"," |-- transaction_hour: long (nullable = true)\n"," |-- customer_name: string (nullable = true)\n"," |-- customer_email: string (nullable = true)\n"," |-- registration_date: string (nullable = true)\n"," |-- customer_tier: string (nullable = true)\n"," |-- last_login_date: string (nullable = true)\n"," |-- is_fraudulent_rule1: boolean (nullable = true)\n","\n","\n","First 5 rows of the PySpark DataFrame:\n","+--------------+-----------+------+-------------------+--------+------------+----------------+-------------+-------------------+-----------------+-------------+---------------+-------------------+\n","|transaction_id|customer_id|amount|          timestamp|currency|  ip_address|transaction_hour|customer_name|     customer_email|registration_date|customer_tier|last_login_date|is_fraudulent_rule1|\n","+--------------+-----------+------+-------------------+--------+------------+----------------+-------------+-------------------+-----------------+-------------+---------------+-------------------+\n","|         TX001|       C101|150.75|2024-07-16 10:00:00|     USD|192.168.1.10|              10|  Alice Smith|  alice@example.com|       2023-01-15|         Gold|     2024-07-15|              false|\n","|         TX002|       C102|  25.0|2024-07-16 10:01:30|     USD|192.168.1.11|              10|  Bob Johnson|    bob@example.com|       2024-07-09|       Silver|     2024-07-16|              false|\n","|         TX003|       C101| 50.25|2024-07-16 10:02:00|     USD|192.168.1.10|              10|  Alice Smith|  alice@example.com|       2023-01-15|         Gold|     2024-07-15|              false|\n","|         TX004|       C103|1200.0|2024-07-16 10:03:15|     EUR|192.168.1.12|              10|Charlie Brown|charlie@example.com|       2023-11-20|       Bronze|     2024-07-14|              false|\n","|         TX005|       C102|  75.5|2024-07-16 10:04:00|     USD|192.168.1.11|              10|  Bob Johnson|    bob@example.com|       2024-07-09|       Silver|     2024-07-16|              false|\n","+--------------+-----------+------+-------------------+--------+------------+----------------+-------------+-------------------+-----------------+-------------+---------------+-------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import datediff\n","\n","try:\n","  sdf_enriched_transactions = sdf_enriched_transactions.withColumn(\n","    \"timestamp_to_date\", to_date(col(\"timestamp\"))).withColumn(\n","    \"registration_date\", to_date(col(\"registration_date\"))\n",")\n","except Exception as e:\n","  print(f\"Error converting dates: {e}\")\n","\n","new_customer = datediff(sdf_enriched_transactions.timestamp_to_date, sdf_enriched_transactions.registration_date) < 7\n","high_transaction = sdf_enriched_transactions.amount > 500\n","\n","sdf_enriched_transactions = sdf_enriched_transactions.withColumn('is_fraudulent_rule1_spark', when(new_customer & high_transaction, True).otherwise(False))\n","sdf_enriched_transactions.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSAAyvin9Gjs","executionInfo":{"status":"ok","timestamp":1752935146077,"user_tz":-120,"elapsed":1036,"user":{"displayName":"Roberto Cerrone","userId":"14489044185171455121"}},"outputId":"10032d31-d390-4ad9-bb66-3665d699400d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------+-----------+------+-------------------+--------+------------+----------------+-------------+-------------------+-----------------+-------------+---------------+-------------------+-----------------+-------------------------+\n","|transaction_id|customer_id|amount|          timestamp|currency|  ip_address|transaction_hour|customer_name|     customer_email|registration_date|customer_tier|last_login_date|is_fraudulent_rule1|timestamp_to_date|is_fraudulent_rule1_spark|\n","+--------------+-----------+------+-------------------+--------+------------+----------------+-------------+-------------------+-----------------+-------------+---------------+-------------------+-----------------+-------------------------+\n","|         TX001|       C101|150.75|2024-07-16 10:00:00|     USD|192.168.1.10|              10|  Alice Smith|  alice@example.com|       2023-01-15|         Gold|     2024-07-15|              false|       2024-07-16|                    false|\n","|         TX002|       C102|  25.0|2024-07-16 10:01:30|     USD|192.168.1.11|              10|  Bob Johnson|    bob@example.com|       2024-07-09|       Silver|     2024-07-16|              false|       2024-07-16|                    false|\n","|         TX003|       C101| 50.25|2024-07-16 10:02:00|     USD|192.168.1.10|              10|  Alice Smith|  alice@example.com|       2023-01-15|         Gold|     2024-07-15|              false|       2024-07-16|                    false|\n","|         TX004|       C103|1200.0|2024-07-16 10:03:15|     EUR|192.168.1.12|              10|Charlie Brown|charlie@example.com|       2023-11-20|       Bronze|     2024-07-14|              false|       2024-07-16|                    false|\n","|         TX005|       C102|  75.5|2024-07-16 10:04:00|     USD|192.168.1.11|              10|  Bob Johnson|    bob@example.com|       2024-07-09|       Silver|     2024-07-16|              false|       2024-07-16|                    false|\n","|         TX006|       C104| 300.0|2024-07-16 10:05:00|     USD|192.168.1.13|              10| Diana Prince|  diana@example.com|       2024-07-10|       Silver|     2024-07-16|              false|       2024-07-16|                    false|\n","|         TX007|       C101|  10.0|2024-07-16 10:06:00|     USD|192.168.1.14|              10|  Alice Smith|  alice@example.com|       2023-01-15|         Gold|     2024-07-15|              false|       2024-07-16|                    false|\n","|         TX008|       C105|   0.0|2024-07-16 10:07:00|     USD|192.168.1.15|              10|    Eve Adams|    eve@example.com|       2023-05-01|         Gold|     2024-07-15|              false|       2024-07-16|                    false|\n","|         TX009|       C102| 200.0|2024-07-16 10:08:00|     USD|192.168.1.11|              10|  Bob Johnson|    bob@example.com|       2024-07-09|       Silver|     2024-07-16|              false|       2024-07-16|                    false|\n","|         TX010|       C106|  45.0|2024-07-16 10:09:00|     USD|192.168.1.16|              10|  Frank White|  frank@example.com|       2024-07-11|       Bronze|     2024-07-15|              false|       2024-07-16|                    false|\n","|         TX011|       C101| 500.0|2024-07-16 10:10:00|     USD|192.168.1.10|              10|  Alice Smith|  alice@example.com|       2023-01-15|         Gold|     2024-07-15|              false|       2024-07-16|                    false|\n","|         TX012|       C107|  20.0|2024-07-16 10:11:00|     USD|192.168.1.17|              10| Grace Hopper|  grace@example.com|       2023-03-22|       Silver|     2024-07-14|              false|       2024-07-16|                    false|\n","|         TX013|       C108|  10.0|2024-07-16 10:12:00|     USD|192.168.1.18|              10|   Heidi Klum|  heidi@example.com|       2024-07-16|       Bronze|     2024-07-16|              false|       2024-07-16|                    false|\n","|         TX014|       C109|  15.0|2024-07-16 10:13:00|     USD|192.168.1.19|              10|   Ivan Drago|   ivan@example.com|       2024-07-16|       Silver|     2024-07-16|              false|       2024-07-16|                    false|\n","|         TX015|       C999| 100.0|2024-07-16 10:14:00|     USD|192.168.1.20|              10|          NaN|                NaN|             NULL|          NaN|            NaN|              false|       2024-07-16|                    false|\n","+--------------+-----------+------+-------------------+--------+------------+----------------+-------------+-------------------+-----------------+-------------+---------------+-------------------+-----------------+-------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["I tried to use UDF (lambda functions) to build the function.\n","\n","```\n","def is_fraudulent_rule1(amount, registration_day, latest_transaction_day):\n","  try:\n","    registration_day = to_date(registration_day)\n","    latest_transaction_day = to_date(latest_transaction_day)\n","  except Exception as e:\n","    print(f\"Error converting dates: {e}\")\n","    return None\n","  \n","  high_transaction = amount > 500\n","  new_customer = datediff(to_date(registration_day), to_date(latest_transaction_day)) < 7\n","  return high_transaction & new_customer\n","\n","sdf_enriched_transactions.withColumn('is_fraudulent_rule1', is_fraudulent_rule1(sdf_enriched_transactions.amount, sdf_enriched_transactions.registration_date, sdf_enriched_transactions.timestamp)).show(5)\n","```\n","\n"],"metadata":{"id":"3Ct3habp7PeU"}},{"cell_type":"code","source":["ds = sdf_enriched_transactions # Use a shorter alias if preferred\n","\n","# Define the base window specification: partition by customer_id, order by timestamp (as Unix timestamp for rangeBetween)\n","# Define the time-based window for counting transactions within 10 minutes (600 seconds)\n","# The frame is from -600 seconds before the current row's timestamp up to the current row (0).\n","windowSpec_10min = Window.partitionBy(ds.customer_id).orderBy(unix_timestamp(ds.timestamp)).rangeBetween(-10 * 60, 0)\n","\n","# Define the window for getting the previous IP address and timestamp (ordered by timestamp)\n","# This window just needs partition and order, the frame defaults to unbounded preceding to current row\n","windowSpec_lag = Window.partitionBy(ds.customer_id).orderBy(ds.timestamp)\n","\n","# Use lag to get the previous transaction's ip_address\n","ds_with_prev = ds.withColumn(\"prev_ip_address\", lag(ds.ip_address, 1).over(windowSpec_lag))\n","\n","# Use count over the 10-minute window to get the transaction count in that period\n","ds_with_counts = ds_with_prev.withColumn(\"transaction_count_10min\", count(\"*\").over(windowSpec_10min))\n","\n","# Implement Rule 2: Multiple Transactions in a Short Period from Different Locations\n","# Logic: Flag if same customer, > 3 transactions in 10 minutes, and ip_address is different from previous.\n","# We need to ensure prev_ip_address is not null for the comparison.\n","sdf_rule2 = ds_with_counts.withColumn(\n","    \"is_fraudulent_rule2\",\n","    when(\n","        (col(\"transaction_count_10min\") > 3) &\n","        (col(\"ip_address\").isNotNull()) &\n","        (col(\"prev_ip_address\").isNotNull()) &\n","        (col(\"ip_address\") != col(\"prev_ip_address\")),\n","        lit(True)\n","    ).otherwise(lit(False))\n",")\n","\n","# Combine fraud flags from both rules (assuming 'is_fraudulent_rule1' or 'is_fraudulent_rule1_pyspark' exists from previous steps)\n","# Let's use 'is_fraudulent_rule1' from the original DataFrame if available, or 'is_fraudulent_rule1_pyspark_reimplemented' if you used that.\n","# Assuming 'is_fraudulent_rule1' is the column you want to use from the input DataFrame for the combined flag.\n","# If you used 'is_fraudulent_rule1_pyspark_reimplemented' from cell ZAc5HTyl_rO_, replace the column name below.\n","sdf_final = sdf_rule2.withColumn(\n","    \"is_fraudulent_combined\",\n","    col(\"is_fraudulent_rule1\") | col(\"is_fraudulent_rule2\")\n",")\n","\n","\n","# Display the relevant columns for transactions flagged by either rule\n","print(\"\\nTransactions flagged by either Rule 1 or Rule 2:\")\n","sdf_final.filter(col(\"is_fraudulent_combined\") == True).select(\n","    \"transaction_id\",\n","    \"customer_id\",\n","    \"timestamp\",\n","    \"ip_address\",\n","    \"is_fraudulent_rule1\", # Display the original Rule 1 flag\n","    \"is_fraudulent_rule2\",\n","    \"is_fraudulent_combined\" # Display combined flag for clarity\n",").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"BuFRjXI2MEH7","executionInfo":{"status":"ok","timestamp":1752935147630,"user_tz":-120,"elapsed":1541,"user":{"displayName":"Roberto Cerrone","userId":"14489044185171455121"}},"outputId":"eea0b316-05eb-4129-cf14-9994251c2ecb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Transactions flagged by either Rule 1 or Rule 2:\n","+--------------+-----------+-------------------+------------+-------------------+-------------------+----------------------+\n","|transaction_id|customer_id|timestamp          |ip_address  |is_fraudulent_rule1|is_fraudulent_rule2|is_fraudulent_combined|\n","+--------------+-----------+-------------------+------------+-------------------+-------------------+----------------------+\n","|TX011         |C101       |2024-07-16 10:10:00|192.168.1.10|false              |true               |true                  |\n","+--------------+-----------+-------------------+------------+-------------------+-------------------+----------------------+\n","\n"]}]},{"cell_type":"code","source":["import os\n","\n","output_path = \"/data/sdf_final_transaction.csv\"\n","\n","# Check if the file already exists\n","if os.path.exists(output_path):\n","    print(f\"File already exists at {output_path}. Overwriting.\")\n","    # In a real scenario, you might want to handle this differently,\n","    # e.g., append, rename, or ask the user for confirmation.\n","\n","# Save the final DataFrame as a CSV file\n","sdf_final.write.csv(output_path, header=True, mode=\"overwrite\")\n","\n","print(f\"DataFrame saved as {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jW69GTTyFJcz","executionInfo":{"status":"ok","timestamp":1752935279779,"user_tz":-120,"elapsed":1145,"user":{"displayName":"Roberto Cerrone","userId":"14489044185171455121"}},"outputId":"195be9e6-eb36-47a0-b8f5-6f37da8f1d77"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["File already exists at /data/sdf_final_transaction.csv. Overwriting.\n","DataFrame saved as /data/sdf_final_transaction.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"77382d2a"},"source":["### Part 2: Conceptual Questions 🤔\n","\n","#### 2.1 Why PySpark?\n","\n","Moving from Pandas to PySpark for a fraud detection system, especially one handling millions of transactions per hour, is driven by the fundamental limitations of Pandas and the architectural advantages of Spark:\n","\n","*   **Pandas Limitations:**\n","    *   **Single-Node Processing:** Pandas operates on data that fits into the memory of a single machine. For large datasets that exceed available RAM, Pandas will fail or become extremely slow.\n","    *   **No Built-in Distributed Processing:** Pandas is not designed for distributed computing. It cannot easily scale across multiple nodes in a cluster to process data in parallel.\n","    *   **Limited Fault Tolerance:** If the single machine running a Pandas script fails, the entire process stops, and you lose your progress.\n","\n","*   **Spark (PySpark) Advantages:**\n","    *   **Distributed Processing:** Spark is designed to process data across a cluster of machines. It breaks down large datasets and computations into smaller tasks that can be executed in parallel on different nodes, enabling it to handle massive amounts of data that would be impossible with Pandas.\n","    *   **Fault Tolerance:** Spark is resilient to failures. If a node in the cluster fails during computation, Spark can recompute the lost partitions of data on other available nodes, ensuring the job completes without losing data.\n","    *   **Lazy Evaluation:** Spark uses lazy evaluation, meaning transformations (like `filter`, `select`, `withColumn`) are not executed immediately. Instead, they build a Directed Acyclic Graph (DAG) of operations. The computation only happens when an action (like `show`, `count`, `collect`) is called. This allows Spark to optimize the execution plan for efficiency.\n","    *   **In-Memory Computing:** Spark can perform computations in memory across the cluster, which is significantly faster than disk-based processing, although it can also spill to disk when data exceeds memory capacity.\n","    *   **Unified Engine:** Spark provides a unified engine for various workloads, including SQL, batch processing, streaming, machine learning, and graph processing, all with a consistent API.\n","\n","For a real-time transaction fraud detection system with a high volume of transactions, PySpark's ability to handle large datasets, distribute processing, and provide fault tolerance is essential, whereas Pandas would quickly become a bottleneck.\n","\n","#### 2.2 Performance Tuning in Spark\n","\n","If a PySpark job is running slowly, here are two optimization techniques:\n","\n","1.  **Data Partitioning:**\n","    *   **Description:** Data in Spark DataFrames is divided into partitions, which are logical chunks of data distributed across the nodes in the cluster. The number and size of partitions significantly impact performance. Too few partitions can lead to data skew (uneven distribution of data across nodes) and limited parallelism. Too many can lead to excessive scheduling overhead.\n","    *   **Improvement:** By repartitioning the DataFrame (e.g., using `repartition()` or `coalesce()`), you can control the number of partitions and ensure data is evenly distributed. This helps to minimize data skew and allows Spark to utilize all available cores effectively for parallel processing, speeding up shuffle-heavy operations like joins and aggregations. Choosing a suitable partitioning strategy (e.g., partitioning by a key used in joins) can also improve performance by reducing data movement during shuffles.\n","\n","2.  **Broadcasting Small DataFrames/Lookups:**\n","    *   **Description:** When performing a join between a large DataFrame and a small DataFrame, Spark typically uses a shuffle hash join or sort merge join, which involves shuffling data across the network. Broadcasting is a technique where a small DataFrame is sent to all executor nodes and stored in memory.\n","    *   **Improvement:** Broadcasting avoids the expensive shuffle operation for the large DataFrame during a join. Instead of sending partitions of the large DataFrame to nodes where the corresponding small DataFrame data resides, the small DataFrame is readily available on all nodes. This significantly reduces network traffic and improves join performance, especially when joining a large fact table with smaller dimension tables. You can hint Spark to broadcast a DataFrame using `pyspark.sql.functions.broadcast()`.\n","\n","#### 2.3 Role of Azure Databricks\n","\n","Azure Databricks simplifies the work done in this exercise in several ways:\n","\n","1.  **Managed Infrastructure:** Azure Databricks provides a fully managed Spark environment. You don't need to manually set up, configure, or manage Spark clusters, including setting up master and worker nodes, installing dependencies, and handling cluster scaling. Databricks handles the underlying infrastructure, allowing you to focus on writing Spark code.\n","2.  **Collaborative Notebooks and Environment:** Databricks offers a collaborative notebook environment similar to Colab but specifically optimized for Spark. It provides integrated support for various languages (Python, Scala, SQL, R), version control integration, and the ability to share notebooks and collaborate with team members seamlessly. The environment is pre-configured with necessary libraries and optimized Spark distributions.\n","3.  **Simplified Job Scheduling and Management:** While you can schedule Spark jobs manually or with external orchestrators, Databricks provides built-in job scheduling capabilities. You can easily schedule notebooks or JARs to run periodically or based on triggers, monitor job runs, and set up alerts within the Databricks platform. This simplifies the operational aspect of running production data pipelines compared to managing scheduling separately.\n","4.  **Integration with Azure Services:** Databricks is deeply integrated with other Azure services. This allows easy access to data stored in Azure Data Lake Storage, Azure Blob Storage, Azure SQL Database, etc., and facilitates building end-to-end data pipelines within the Azure ecosystem. Integration with services like Azure Active Directory also simplifies security and access control."]}]}